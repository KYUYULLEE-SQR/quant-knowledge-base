# Common Pitfalls in Quant Research (í€€íŠ¸ì˜ í•¨ì •)

**Last Updated**: 2025-12-22
**Source**: User experience, industry best practices, academic research
**Importance**: â­â­â­ Critical - í•œ ë²ˆ ë¹ ì§€ë©´ ëª‡ ì£¼/ëª‡ ë‹¬ ë‚­ë¹„

---

## Overview

**Quant pitfall** = ë°±í…ŒìŠ¤íŠ¸ì—ì„œ ì¢‹ì•„ ë³´ì´ì§€ë§Œ ì‹¤ê±°ë˜ì—ì„œ ì‹¤íŒ¨í•˜ëŠ” ì›ì¸

**Why Critical**:
- ë°±í…ŒìŠ¤íŠ¸ Sharpe 3.0 â†’ ì‹¤ê±°ë˜ -0.5 (ìì£¼ ë°œìƒ)
- ì›ì¸ ëŒ€ë¶€ë¶„: ì´ ë¬¸ì„œì— ë‚˜ì—´ëœ í•¨ì •ë“¤
- **ì˜ˆë°© ê°€ëŠ¥**: ì œëŒ€ë¡œ ì•Œë©´ í”¼í•  ìˆ˜ ìˆìŒ

---

## 1. Look-Ahead Bias (ë¯¸ë˜ì°¸ì¡°) â­â­â­

### Definition

**ë¯¸ë˜ ì •ë³´ë¥¼ í˜„ì¬ ì‹œì  ê²°ì •ì— ì‚¬ìš©**í•˜ëŠ” ì˜¤ë¥˜

**Why Fatal**:
- ë°±í…ŒìŠ¤íŠ¸ì—ì„œ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ìµ ì°½ì¶œ
- ì‹¤ê±°ë˜ì—ì„œ ì ˆëŒ€ ì¬í˜„ ë¶ˆê°€ëŠ¥
- ê°€ì¥ í”í•˜ê³  ê°€ì¥ ì¹˜ëª…ì ì¸ í•¨ì •

### Common Cases

#### Case 1: ë°ì´í„° shift ì‹¤ìˆ˜

**âŒ Bad (Look-ahead bias)**:
```python
# ì˜¤ëŠ˜ ê°€ê²©ìœ¼ë¡œ ë‚´ì¼ ì‹ í˜¸ ìƒì„±
signals = df['close'].shift(-1) > df['close']  # ë‚´ì¼ ê°€ê²© ì‚¬ìš©! âŒ

# ì‹ í˜¸ì™€ ìˆ˜ìµ íƒ€ì´ë° ë¶ˆì¼ì¹˜
df['signal'] = (df['close'] > df['close'].shift(1))  # ì˜¤ëŠ˜ ì¢…ê°€ë¡œ íŒë‹¨
df['returns'] = df['close'].pct_change()  # ì˜¤ëŠ˜ ì¢…ê°€ ê¸°ì¤€ ìˆ˜ìµ âŒ
# â†’ ì‹ í˜¸ ìƒì„± ì‹œì (ì¢…ê°€)ê³¼ ì§„ì… ì‹œì (ì¢…ê°€) ê°™ìŒ = ë¶ˆê°€ëŠ¥
```

**âœ… Good (Correct)**:
```python
# ì–´ì œ ì¢…ê°€ë¡œ ì˜¤ëŠ˜ ì‹ í˜¸ ìƒì„±
signals = df['close'].shift(1) > df['close'].shift(2)  # ê³¼ê±° ì •ë³´ë§Œ

# ì‹ í˜¸ëŠ” t-1, ìˆ˜ìµì€ t
df['signal'] = (df['close'].shift(1) > df['close'].shift(2))
df['returns'] = df['close'].pct_change()  # ì˜¤ëŠ˜ ìˆ˜ìµ
# â†’ t-1 ì¢…ê°€ ë³´ê³  íŒë‹¨ â†’ t ì¢…ê°€ì— ì§„ì…/ì²­ì‚° âœ…
```

#### Case 2: Rolling window ì¤‘ì•™ ì •ë ¬

**âŒ Bad (Look-ahead bias)**:
```python
# center=True â†’ ë¯¸ë˜ ë°ì´í„° í¬í•¨
df['ma_20'] = df['close'].rolling(20, center=True).mean()  # âŒ
df['signal'] = df['close'] > df['ma_20']
# â†’ ì¤‘ì•™ ì •ë ¬ = ë¯¸ë˜ 10ì¼ ë°ì´í„° ì‚¬ìš©
```

**âœ… Good**:
```python
# center=False (default) â†’ ê³¼ê±° ë°ì´í„°ë§Œ
df['ma_20'] = df['close'].rolling(20, center=False).mean()  # âœ…
df['signal'] = df['close'] > df['ma_20']
```

#### Case 3: Featureì— ë¯¸ë˜ ë°ì´í„°

**âŒ Bad (Options strategy)**:
```python
# Settlement priceëŠ” ë§Œê¸°ì¼ì—ë§Œ ì•Œ ìˆ˜ ìˆìŒ!
df['fair_iv'] = calculate_iv(
    settlement_price=df['settlement_price'],  # ë¯¸ë˜ ì •ë³´! âŒ
    strike=df['strike'],
    dte=df['dte']
)
```

**âœ… Good**:
```python
# í˜„ì¬ mark priceë§Œ ì‚¬ìš©
df['fair_iv'] = calculate_iv(
    current_price=df['mark_price'],  # í˜„ì¬ ì •ë³´ âœ…
    strike=df['strike'],
    dte=df['dte']
)
```

#### Case 4: Resampling í›„ ffill/bfill

**âŒ Bad**:
```python
# 1ë¶„ â†’ 1ì‹œê°„ ë¦¬ìƒ˜í”Œë§
df_hourly = df_minute.resample('1H').last()
df_hourly['filled'] = df_hourly['close'].fillna(method='bfill')  # ë¯¸ë˜ ì±„ì›€! âŒ
```

**âœ… Good**:
```python
# Forward fill only (ê³¼ê±°ë¡œ ì±„ì›€)
df_hourly = df_minute.resample('1H').last()
df_hourly['filled'] = df_hourly['close'].fillna(method='ffill')  # âœ…
```

### Detection Methods

#### 1. Signal Shift Test (Placebo Test)

**ì›ë¦¬**: ì‹ í˜¸ë¥¼ +1 bar shift â†’ alpha ì‚¬ë¼ì ¸ì•¼ ì •ìƒ

```python
def test_look_ahead_bias(backtest_func, data):
    """
    Test for look-ahead bias by shifting signal.

    If alpha remains after shift â†’ look-ahead bias detected.
    """
    # Original backtest
    original_sharpe = backtest_func(data, shift=0)

    # Shifted backtest (+1 bar)
    shifted_sharpe = backtest_func(data, shift=1)

    print(f"Original Sharpe: {original_sharpe:.2f}")
    print(f"Shifted (+1) Sharpe: {shifted_sharpe:.2f}")

    if abs(shifted_sharpe) > 0.5:  # Alpha should disappear
        print("âš ï¸ WARNING: Possible look-ahead bias detected!")
        print("   Alpha remains after signal shift.")
        return False
    else:
        print("âœ… No look-ahead bias detected.")
        return True

# Usage
is_clean = test_look_ahead_bias(my_backtest, data)
```

#### 2. Manual Code Review

**Checklist**:
- [ ] ëª¨ë“  `.shift()` í™•ì¸: negative shift ì—†ëŠ”ê°€?
- [ ] Rolling window: `center=False` ì¸ê°€?
- [ ] Feature ìƒì„±: ë¯¸ë˜ ë°ì´í„° ì•ˆ ì“°ëŠ”ê°€?
- [ ] Label ìƒì„±: íƒ€ì´ë° ëª…í™•í•œê°€? (t ì •ë³´ë¡œ t+1 ì˜ˆì¸¡?)

#### 3. Walk-Forward Validation

**ì›ë¦¬**: í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—„ê²©íˆ ë¶„ë¦¬

```python
# Train on 2023, test on 2024
train_data = data['2023-01-01':'2023-12-31']
test_data = data['2024-01-01':'2024-12-31']

# Model trained ONLY on train_data
model.fit(train_data)

# Test on unseen data
test_sharpe = backtest(model, test_data)
```

---

## 2. Selection Bias (ì„ íƒ í¸í–¥) â­â­â­

### Definition

**ì‚´ì•„ë‚¨ì€ ê²ƒë§Œ** ì„ íƒí•´ì„œ ë¶„ì„í•˜ëŠ” ì˜¤ë¥˜

### Common Cases

#### Case 1: Survivorship Bias (ìƒì¡´ í¸í–¥)

**âŒ Bad**:
```python
# í˜„ì¬ ìƒì¥ëœ ì£¼ì‹ë§Œ ë¶„ì„
tickers = ['AAPL', 'MSFT', 'GOOGL', ...]  # í˜„ì¬ S&P 500
# â†’ ë§í•œ íšŒì‚¬ ì œì™¸ë¨ (Enron, Lehman, ...)
# â†’ Sharpe ì¸í”Œë ˆì´ì…˜

data = download_data(tickers, start='2000-01-01')
backtest_result = backtest(data)  # ê³¼ëŒ€í‰ê°€ëœ ì„±ê³¼ âŒ
```

**âœ… Good**:
```python
# ê³¼ê±° ì‹œì ì— ìƒì¥ëœ ëª¨ë“  ì£¼ì‹ (delisted í¬í•¨)
tickers = get_universe_at_date('2000-01-01', include_delisted=True)
# â†’ ë§í•œ íšŒì‚¬ë„ í¬í•¨

data = download_data(tickers, start='2000-01-01')
backtest_result = backtest(data)  # í˜„ì‹¤ì  ì„±ê³¼ âœ…
```

**Options equivalent**:
```python
# âŒ Bad: ë§Œê¸°ê¹Œì§€ ì‚´ì•„ë‚¨ì€ ì˜µì…˜ë§Œ
options = query("SELECT * FROM options WHERE expired=True AND pnl IS NOT NULL")

# âœ… Good: ë„ì¤‘ì— delisted/ì²­ì‚°ëœ ê²ƒë„ í¬í•¨
options = query("SELECT * FROM options WHERE listed_date < backtest_end")
```

#### Case 2: Cherry-Picking (ì²´ë¦¬í”¼í‚¹)

**âŒ Bad**:
```python
# ì—¬ëŸ¬ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ â†’ ì¢‹ì€ ê²ƒë§Œ ì„ íƒ
results = []
for ma_period in range(5, 50):
    sharpe = backtest(ma_period=ma_period)
    results.append((ma_period, sharpe))

# ìµœê³  ì„±ê³¼ë§Œ ì„ íƒ
best_period = max(results, key=lambda x: x[1])[0]  # âŒ
print(f"Best MA period: {best_period}")
# â†’ ì´ íŒŒë¼ë¯¸í„°ëŠ” ê³¼ê±° ë°ì´í„°ì— overfittingë¨
```

**âœ… Good**:
```python
# Out-of-sample í…ŒìŠ¤íŠ¸
train_data = data['2020':'2022']
test_data = data['2023':'2024']

# Train periodì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
best_period = optimize_parameter(train_data)

# Test periodì—ì„œ ê²€ì¦ (ìƒˆë¡œìš´ ë°ì´í„°)
test_sharpe = backtest(test_data, ma_period=best_period)

if test_sharpe > threshold:
    print(f"âœ… Robust: {best_period}")
else:
    print(f"âŒ Overfit: {best_period}")
```

#### Case 3: Specific Period Selection

**âŒ Bad**:
```python
# "ì¢‹ì€ ê¸°ê°„"ë§Œ ì„ íƒ
backtest_data = data['2020-03-01':'2021-12-31']  # Bull market only âŒ
sharpe = backtest(backtest_data)
# â†’ íŠ¹ì • ì‹œì¥ êµ­ë©´ì—ì„œë§Œ ì‘ë™
```

**âœ… Good**:
```python
# ì „ì²´ ê¸°ê°„ í…ŒìŠ¤íŠ¸ (Bull + Bear + Sideways)
backtest_data = data['2015-01-01':'2024-12-31']  # 10 years âœ…
sharpe = backtest(backtest_data)

# êµ¬ê°„ë³„ ì„±ê³¼ ë¶„ì„
for period, label in [('2015-2017', 'Bull'), ('2018', 'Bear'), ('2019-2021', 'Bull'), ('2022', 'Bear')]:
    period_sharpe = backtest(data[period])
    print(f"{label}: Sharpe {period_sharpe:.2f}")

# ëª¨ë“  êµ¬ê°„ì—ì„œ ì‘ë™í•´ì•¼ robust
```

### Detection Methods

#### 1. Universe Consistency Check

```python
def check_survivorship_bias(data, start_date):
    """
    Check if universe includes delisted instruments.
    """
    # Get current universe
    current_tickers = set(data.columns)

    # Get historical universe (should be larger)
    historical_tickers = get_universe_at_date(start_date, include_delisted=True)

    missing = historical_tickers - current_tickers
    missing_count = len(missing)

    if missing_count > 0:
        print(f"âš ï¸ WARNING: {missing_count} delisted tickers missing")
        print(f"   Examples: {list(missing)[:10]}")
        return False
    else:
        print(f"âœ… Universe complete ({len(current_tickers)} tickers)")
        return True
```

#### 2. Parameter Stability Test

```python
def test_parameter_stability(backtest_func, param_range):
    """
    Test if performance is stable around optimal parameter.

    If only one parameter value works â†’ overfitting.
    """
    results = []
    for param in param_range:
        sharpe = backtest_func(param)
        results.append((param, sharpe))

    # Check stability
    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)
    best_sharpe = sorted_results[0][1]
    second_sharpe = sorted_results[1][1]

    stability = (best_sharpe - second_sharpe) / best_sharpe

    if stability > 0.3:  # >30% drop
        print(f"âš ï¸ WARNING: Unstable parameter")
        print(f"   Best: {sorted_results[0]}")
        print(f"   2nd: {sorted_results[1]}")
        return False
    else:
        print(f"âœ… Stable parameter (Â±10-20% performance)")
        return True
```

---

## 3. Data Snooping (ë°ì´í„° ìŠ¤ëˆ„í•‘) â­â­

### Definition

**ê°™ì€ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ë²ˆ ì‹¤í—˜** â†’ ìš°ì—°íˆ ì¢‹ì€ ê²°ê³¼ ì„ íƒ

**The Problem**:
- 100ê°œ ì „ëµ í…ŒìŠ¤íŠ¸ â†’ 5ê°œ Sharpe > 2.0 ë°œê²¬
- ì´ ì¤‘ ì§„ì§œ alpha? 0-1ê°œ (ë‚˜ë¨¸ì§€ëŠ” ìš´)
- Multiple testing problem

### Prevention

#### 1. One Hypothesis Per Experiment

```python
# âŒ Bad: ì—¬ëŸ¬ ê°€ì„¤ ë™ì‹œ í…ŒìŠ¤íŠ¸
for feature in ['ma', 'rsi', 'macd', 'bbands', ...]:  # 100 features
    for window in range(5, 100):  # 95 windows
        sharpe = backtest(feature, window)
        if sharpe > 2.0:
            print(f"Found alpha: {feature}, {window}")  # âŒ False discovery
```

```python
# âœ… Good: í•˜ë‚˜ì˜ ê°€ì„¤ í…ŒìŠ¤íŠ¸
# Experiment: "MA crossover works?"
feature = 'ma_crossover'
window = 20  # Pre-specified (not optimized)
sharpe = backtest(feature, window)

# Result: Accept or reject hypothesis
# â†’ ë‹¤ìŒ ì‹¤í—˜: ë‹¤ë¥¸ ê°€ì„¤ (ìƒˆë¡œìš´ ë°ì´í„° or out-of-sample)
```

#### 2. Bonferroni Correction

**ì›ë¦¬**: Multiple tests â†’ ìœ ì˜ìˆ˜ì¤€ ì¡°ì •

```python
def bonferroni_test(sharpes, alpha=0.05):
    """
    Adjust significance level for multiple tests.

    Args:
        sharpes: List of Sharpe ratios from N experiments
        alpha: Desired significance level (e.g., 0.05)

    Returns:
        significant: List of truly significant strategies
    """
    n_tests = len(sharpes)
    adjusted_alpha = alpha / n_tests  # Bonferroni correction

    # Critical Sharpe for adjusted alpha
    # (Simplified: assume Sharpe ~ N(0, 1))
    from scipy import stats
    critical_sharpe = stats.norm.ppf(1 - adjusted_alpha/2)

    significant = [s for s in sharpes if abs(s) > critical_sharpe]

    print(f"Total tests: {n_tests}")
    print(f"Adjusted alpha: {adjusted_alpha:.4f}")
    print(f"Critical Sharpe: {critical_sharpe:.2f}")
    print(f"Significant strategies: {len(significant)}/{n_tests}")

    return significant
```

#### 3. Hold-Out Set (ì™„ì „ ë¶„ë¦¬)

```python
# âŒ Bad: ëª¨ë“  ë°ì´í„°ë¡œ ìµœì í™”
full_data = data['2015':'2024']
best_strategy = optimize(full_data)  # Overfit âŒ

# âœ… Good: Hold-out setìœ¼ë¡œ ìµœì¢… ê²€ì¦
train_data = data['2015':'2022']  # 80%
holdout_data = data['2023':'2024']  # 20% (ì™„ì „íˆ ë³„ë„)

# Trainì—ì„œ ê°œë°œ
best_strategy = optimize(train_data)

# Hold-outì—ì„œ 1íšŒë§Œ í…ŒìŠ¤íŠ¸ (no ì¬ìµœì í™”)
final_sharpe = backtest(holdout_data, best_strategy)

if final_sharpe > threshold:
    print("âœ… Strategy validated")
else:
    print("âŒ Strategy failed (overfit)")
```

---

## 4. Transaction Cost Underestimation â­â­â­

### Why Common

**Backtest assumptions**:
- Zero slippage
- Instant fill
- Maker fee only
- No partial fills

**Reality**:
- 2-10 bps slippage
- Partial fills (30%)
- Mixed maker/taker
- Reorder delays

### Solution

ğŸ“š **ì¶œì²˜**: [Transaction Cost Model](../modeling/transaction_cost_model.md)

**Key Points**:
- T-cost = fees + slippage + partial fill impact
- Test at 0.5Ã—, 1Ã—, 2Ã— costs
- If Sharpe < 0 at 2Ã— â†’ too cost-sensitive

---

## 5. Overfitting (ê³¼ìµœì í™”) â­â­â­

### Definition

**ê³¼ê±° ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ë§ì¶¤** â†’ ë¯¸ë˜ì— ì‘ë™ ì•ˆ í•¨

### Symptoms

1. **Too many parameters** (>5-10)
2. **Perfect backtest** (Sharpe > 5, MDD < 5%)
3. **Parameter sensitivity** (Â±10% change â†’ Sharpe 50% drop)
4. **Complex rules** (if-then-else 10+ levels)

### Prevention

#### 1. Regularization (Model)

```python
# âŒ Bad: Complex model, no regularization
model = RandomForestRegressor(
    n_estimators=500,
    max_depth=50,  # Very deep
    min_samples_split=2  # No pruning
)

# âœ… Good: Regularized
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,  # Shallow (prevent overfit)
    min_samples_split=20,  # More pruning
    max_features='sqrt'  # Feature sampling
)
```

#### 2. Cross-Validation (Time-Series)

```python
# âŒ Bad: Random CV (look-ahead bias)
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)  # Random split âŒ

# âœ… Good: Time-series CV (no look-ahead)
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
scores = cross_val_score(model, X, y, cv=tscv)  # âœ…
```

#### 3. Simplicity Bias

**Rule**: Simpler = Better (if similar performance)

```
Strategy A: Sharpe 2.5, 20 parameters, 500 lines code
Strategy B: Sharpe 2.3, 3 parameters, 50 lines code

â†’ Choose B (more robust, less overfit)
```

---

## 6. Backtest-Reality Gap â­â­

### Common Gaps

1. **Data quality**: Backtest (clean) vs Live (missing, outliers)
2. **Execution**: Backtest (perfect) vs Live (delays, rejects)
3. **Market impact**: Backtest (price taker) vs Live (you move market)
4. **Regime change**: Backtest (past) vs Live (future = different)

### Mitigation

#### Paper Trading (í•„ìˆ˜)

```
Backtest â†’ Paper Trading (2-4 weeks) â†’ Live Trading

Paper trading:
  - Real market data
  - Real execution (simulated)
  - Real delays
  - No real money

Metrics to check:
  - Fill rate: Backtest 100% â†’ Paper 70-90%
  - Sharpe: Backtest 2.5 â†’ Paper 2.0-2.3 (acceptable)
  - Trade count: Backtest 100 â†’ Paper 80-95
```

#### Slippage Logging

```python
# In live trading
def execute_order(order):
    expected_price = order.price
    actual_price = exchange.fill_order(order)

    slippage_bps = abs(actual_price - expected_price) / expected_price * 10000

    logger.info(f"Order filled: Expected {expected_price}, "
                f"Actual {actual_price}, Slippage {slippage_bps:.1f} bps")

    # Alert if slippage too high
    if slippage_bps > 20:
        logger.warning(f"HIGH SLIPPAGE: {slippage_bps:.1f} bps")
```

---

## 7. Regime Change Ignorance â­â­

### Definition

**ì‹œì¥ êµ¬ì¡° ë³€í™”** ë¬´ì‹œ â†’ ê³¼ê±° ì „ëµ ë¯¸ë˜ì— ì‘ë™ ì•ˆ í•¨

### Examples

- **2020-2021**: Low vol, bull â†’ Momentum works
- **2022**: High vol, bear â†’ Reversion works
- **2023-2024**: Choppy â†’ Range-bound strategies

### Solution

#### Regime-Aware Backtest

```python
def backtest_by_regime(data, strategy):
    """
    Test strategy performance by market regime.

    Regime classification (simple):
      - Bull: +20% trailing 6M
      - Bear: -20% trailing 6M
      - Sideways: else
    """
    data['regime'] = classify_regime(data)

    results = {}
    for regime in ['bull', 'bear', 'sideways']:
        regime_data = data[data['regime'] == regime]
        sharpe = backtest(regime_data, strategy)
        results[regime] = sharpe

    print("Performance by Regime:")
    for regime, sharpe in results.items():
        print(f"  {regime.capitalize()}: Sharpe {sharpe:.2f}")

    # Strategy is robust if works in all regimes
    if min(results.values()) > 1.0:
        print("âœ… Regime-robust strategy")
    else:
        print("âš ï¸ Regime-dependent strategy")

    return results
```

---

## Summary Table

| Pitfall | Detection | Prevention |
|---------|-----------|-----------|
| **Look-ahead bias** | Signal shift test, Code review | Strict time separation, Placebo test |
| **Selection bias** | Universe check, Period analysis | Include delisted, Full period test |
| **Data snooping** | Multiple testing correction | One hypothesis per experiment, Hold-out set |
| **Low t-cost** | Cost sensitivity test | Realistic costs (7-23 bps), 2Ã— test |
| **Overfitting** | Parameter stability, CV | Regularization, Simplicity bias |
| **Backtest gap** | Paper trading | Paper 2-4 weeks, Slippage logging |
| **Regime change** | Regime-split analysis | Test all regimes, Adaptive strategies |

---

## Agent Protocol: Pitfall Check

**BEFORE finalizing strategy:**

1. âœ… Read this file (common_pitfalls.md)
2. âœ… Run **all** detection tests:
   - Signal shift (look-ahead)
   - Cost sensitivity (2Ã—)
   - Parameter stability (Â±20%)
   - Regime split
3. âœ… Document which pitfalls checked (in README)
4. âœ… If any test fails â†’ FIX, don't ignore
5. âœ… Recommend paper trading before live

**Example**:
```
User: "ì´ ì „ëµ ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œí–ˆì–´"

Agent:
  [Reads common_pitfalls.md]

  "ë°±í…ŒìŠ¤íŠ¸ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:

   âš ï¸ í•„ìˆ˜ í™•ì¸ í•­ëª©:
   1. Look-ahead bias test (signal shift +1)
   2. Transaction cost 2Ã— test
   3. Parameter stability (Â±20%)
   4. Regime êµ¬ê°„ë³„ ì„±ê³¼

   í˜„ì¬ ìƒíƒœ:
   - Sharpe: 2.5 (ì¢‹ìŒ)
   - í•˜ì§€ë§Œ ê²€ì¦ ì•ˆ ë¨ â†’ ì‹¤ê±°ë˜ ìœ„í—˜

   ì§„í–‰ ë°©ë²•:
   1. ìœ„ 4ê°€ì§€ í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰
   2. ëª¨ë‘ í†µê³¼ â†’ Paper trading 2ì£¼
   3. Paper ì„±ê³¼ OK â†’ Live ì†Œì•¡ ë°°í¬

   ğŸ“š ì¶œì²˜: knowledge/experiments/common_pitfalls.md

   ì§€ê¸ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘í• ê¹Œìš”?"
```

---

## References

- **Advances in Financial Machine Learning** (Marcos LÃ³pez de Prado) - Data snooping, overfitting
- **Related KB**:
  - [Experiment Methodology](methodology.md) - One variable at a time
  - [Transaction Cost Model](../modeling/transaction_cost_model.md) - Realistic costs
  - [Lessons Learned](lessons_learned.md) - Actual failure cases
- **Academic**: Bailey et al. (2014) "The Probability of Backtest Overfitting"

---

**Version**: 1.0
**Critical**: Every pitfall in this list has caused real money loss. Take seriously.
